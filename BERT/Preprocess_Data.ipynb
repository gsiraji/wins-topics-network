{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74887317",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a63eae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3a5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_dataset = pd.read_parquet('./arxiv_nlp.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483d4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_dataset = pd.read_parquet('./arxiv_climate_change.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399737a",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d87f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/moyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377b0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare stopwords\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8acc9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_abs = cc_dataset.abstract.values.tolist()\n",
    "cc_ttl = cc_dataset.title.values.tolist()\n",
    "nlp_abs = nlp_dataset.abstract.values.tolist()\n",
    "nlp_ttl = nlp_dataset.title.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd044a4d",
   "metadata": {},
   "source": [
    "## Clean texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb578468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "# Tokenize words and cleanup the text\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            \n",
    "    #deacc=True removes punctuations\n",
    "\n",
    "# Remove Stopwords, make bigrams and lemmatize\n",
    "# Define function for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    # Build the bigram model\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "\n",
    "    # Faster way to get a sentence clubbed as a bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def text_preprocess(docs): \n",
    "    # docs: list of str\n",
    "      \n",
    "    # Remove new line characters \n",
    "    docs = [re.sub('\\s+', ' ', doc) for doc in docs]  \n",
    "    # Remove distracting single quotes \n",
    "    docs = [re.sub(\"\\'\", \"\", doc) for doc in docs]\n",
    "\n",
    "    docs_words = list(sent_to_words(docs))\n",
    "\n",
    "    # Call preprocessing functions in order\n",
    "    # Remove Stop Words\n",
    "    docs_words_nostops = remove_stopwords(docs_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    docs_words_bigrams = make_bigrams(docs_words_nostops)\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    docs_lemmatized = lemmatization(docs_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # Create Corpus \n",
    "    docs_cleaned = docs_lemmatized\n",
    "    \n",
    "    return docs_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d0d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_abs_words_cleaned = text_preprocess(cc_abs)\n",
    "cc_abs_cleaned = list(map(lambda x: ' '.join(x), cc_abs_words_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c92600",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_ttl_words_cleaned = text_preprocess(cc_ttl)\n",
    "cc_ttl_cleaned = list(map(lambda x: ' '.join(x), cc_ttl_words_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0cf6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_abs_words_cleaned = text_preprocess(nlp_abs)\n",
    "nlp_abs_cleaned = list(map(lambda x: ' '.join(x), nlp_abs_words_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5961df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ttl_words_cleaned = text_preprocess(nlp_ttl)\n",
    "nlp_ttl_cleaned = list(map(lambda x: ' '.join(x), nlp_ttl_words_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a361d8",
   "metadata": {},
   "source": [
    "## save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "194d927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148e15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_dic = {\"climate_change_abstract\":cc_abs_cleaned, \"climate_change_title\":cc_ttl_cleaned, \"nlp_abstract\": nlp_abs_cleaned, \"nlp_title\": nlp_ttl_cleaned}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ccfbd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocessed_data.json\", 'w', encoding='utf-8') as fl:\n",
    "    json.dump(save_data_dic, fl, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c7269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
