{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tfai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2023-05-05 11:20:48.938752: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# from https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "from pprint import pprint# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel# spaCy for preprocessing\n",
    "import spacy# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import parquet\n",
    "import os\n",
    "\n",
    "# Prepare stopwords\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    # Build the bigram\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    # Faster way to get a sentence clubbed as a bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    # Build the trigram models\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # See trigram example\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    # nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: DeprecationWarning: invalid escape sequence '\\s'\n",
      "<>:25: DeprecationWarning: invalid escape sequence '\\s'\n",
      "/var/folders/nk/cmv723pj6n38_mwnlhj_7z3m0000gp/T/ipykernel_98016/81130803.py:25: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words and cleanup the text\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            \n",
    "    #deacc=True removes punctuations\n",
    "\n",
    "def preprocess(dataFrameName):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "    # load daaset\n",
    "    # path = os.getcwd()\n",
    "    # df = pd.read_parquet('{}/project_data/arxiv_climate_change.parquet'.format(path), engine='auto')\n",
    "    \n",
    "    \n",
    "    filePath = '%s.parquet' % (dataFrameName)\n",
    "\n",
    "    df = pd.read_parquet(filePath)\n",
    "\n",
    "    # Remove newline characters\n",
    "    # Convert to list \n",
    "    data = df.abstract.values.tolist()  \n",
    "    # Remove new line characters \n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]  \n",
    "    # Remove distracting single quotes \n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]  \n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    # Call preprocessing functions in order\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    \n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # print(data_lemmatized[:1])\n",
    "\n",
    "    return data_lemmatized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary and Corpus needed for Topic Modeling\n",
    "    # Create Dictionary \n",
    "\n",
    "def get_corpus(df):\n",
    "    data_lemmatized = preprocess(df)\n",
    "    id2word = corpora.Dictionary(data_lemmatized)  \n",
    "        # Create Corpus \n",
    "    texts = data_lemmatized  \n",
    "        # Term Document Frequency \n",
    "    corpus = [id2word.doc2bow(text) for text in texts]  \n",
    "\n",
    "    return(corpus, id2word,data_lemmatized)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(corpus, id2word,data_lemmatized):\n",
    "    # Building topic model\n",
    "    \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=16, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1, # determines how often the model parameters should be updated\n",
    "                                            chunksize=100, # the number of documents to be used in each training chunk\n",
    "                                            passes=10, # the total number of training passes\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "\n",
    "    # Print the keyword of topics\n",
    "    # pprint(lda_model.print_topics())\n",
    "    # doc_lda = lda_model[corpus]\n",
    "    # # Evaluate topic models\n",
    "\n",
    "    # # Compute model Perplexity and Coherence score\n",
    "    # # Compute Perplexity\n",
    "    # # print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "    # # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # # Compute Coherence Score\n",
    "    # coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    # coherence_lda = coherence_model_lda.get_coherence()\n",
    "    # print('\\nCoherence Score: ', coherence_lda)\n",
    "    #Higher the topic coherence, the topic is more human interpretable.\n",
    "    return(lda_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_topic_dataframes(ldaModel,corpus,dataFrameName,year):\n",
    "    all_topics = ldaModel.get_document_topics(corpus, per_word_topics=True)\n",
    "\n",
    "    doc_list = []\n",
    "    for doc_topics, word_topics, phi_values in all_topics:\n",
    "        doc_list.append(doc_topics)\n",
    "\n",
    "    # create documents x topic df\n",
    "    df = pd.DataFrame(doc_list)\n",
    "\n",
    "    df_all = df[0]\n",
    "    df_all = df_all.reset_index()\n",
    "    df_all = df_all.rename({'index': 'document', 0: 'tw' }, axis=1)\n",
    "\n",
    "    for val in df.columns:\n",
    "        dval = df[val]\n",
    "        dval = dval.reset_index()\n",
    "        dval = dval.rename({'index': 'document', val: 'tw' }, axis=1)\n",
    "        df_all = pd.concat([df_all,dval])\n",
    "\n",
    "    df_all[['topic', 'weight']] = pd.DataFrame(df_all['tw'].tolist(), index=df_all.index)\n",
    "    df_all.drop('tw', axis = 1)\n",
    "    df_all = df_all.dropna()\n",
    "    df_all['topic'] = df_all['topic'].astype(int)\n",
    "\n",
    "    df_doc_topic = df_all.pivot_table(index='document', \n",
    "                            columns='topic', \n",
    "                            values='weight')\n",
    "\n",
    "    # rename topic columns T#\n",
    "    for val in df.columns:\n",
    "        df_doc_topic = df_doc_topic.rename({val: 'T{}'.format(val) }, axis=1)\n",
    "\n",
    "    df_doc_topic = df_all.pivot_table(index='document', \n",
    "                            columns='topic', \n",
    "                            values='weight')\n",
    "\n",
    "    # save\n",
    "    path = os.getcwd()\n",
    "    # df_doc_topic.to_csv(\"{}/subset_data/document_topic_{}_{}.csv\".format(path,dataFrameName,year))\n",
    "\n",
    "\n",
    "    # create df with top 3 words for each topic\n",
    "    x = ldaModel.show_topics(num_topics=16, num_words=3,formatted=False)\n",
    "    topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "    # print(topics_words)\n",
    "    df_words = pd.DataFrame(topics_words)\n",
    "    df_words = df_words.rename({0: 'Topic', 1: 'words' }, axis=1)\n",
    "\n",
    "    # change all values > 0 to 1\n",
    "    df_doc_topic_1s = df_doc_topic\n",
    "    for col in df_doc_topic_1s.columns:\n",
    "        df_doc_topic_1s.loc[df_doc_topic_1s[col] > 0, col] = 1\n",
    "\n",
    "    # create topic freq df\n",
    "    df_topic_freq = pd.DataFrame()\n",
    "    for val1 in df_doc_topic_1s.columns:\n",
    "        df_sub = df_doc_topic.groupby([val1]).size().reset_index(name=\"frequency\")\n",
    "        df_sub[\"Topic\"] = val1\n",
    "        df_sub = df_sub.drop([val1], axis = 1)\n",
    "\n",
    "        df_topic_freq = pd.concat([df_sub,df_topic_freq])\n",
    "\n",
    "    # add top 3 words\n",
    "    df_topic_freq = df_topic_freq.merge(df_words, how='left', on='Topic')\n",
    "\n",
    "    # save\n",
    "    df_topic_freq.to_csv(\"{}/subset_data/topic_freq_{}_{}.csv\".format(path,dataFrameName,year))\n",
    "\n",
    "    # create topic x topic freq df\n",
    "    df_topic = pd.DataFrame()\n",
    "    for val1 in df_doc_topic_1s.columns:\n",
    "        for val2 in df_doc_topic_1s.columns:\n",
    "            if val1 == val2:\n",
    "                x = 1\n",
    "            else:\n",
    "                df_sub = df_doc_topic.groupby([val1, val2]).size().reset_index(name=\"frequency\")\n",
    "                df_sub[\"TopicA\"] = val1\n",
    "                df_sub[\"TopicB\"] = val2\n",
    "                df_sub = df_sub.drop([val1,val2], axis = 1)\n",
    "\n",
    "                df_topic = pd.concat([df_sub,df_topic])\n",
    "\n",
    "    # add top 3 words\n",
    "    df_wordsA = df_words.rename({\"Topic\": \"TopicA\", \"words\": \"TopicA_words\"}, axis=1)\n",
    "    df_topic = df_topic.merge(df_wordsA, how='left', on='TopicA')\n",
    "    df_wordsB = df_words.rename({\"Topic\": \"TopicB\", \"words\": \"TopicB_words\"}, axis=1)\n",
    "    df_topic = df_topic.merge(df_wordsB, how='left', on='TopicB')\n",
    "\n",
    "    # save\n",
    "    df_topic.to_csv(\"{}/subset_data/topic_topic_freq_{}_{}.csv\".format(path,dataFrameName,year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topics_code_for_subsets(dataFrameName,years):\n",
    "    for year in years:\n",
    "        fileName = 'subset_data/{}_{}'.format(dataFrameName,year)\n",
    "        \n",
    "        corpus, id2word,data_lemmatized = get_corpus(fileName)\n",
    "        pprint('year {} corpus done!'.format(year))\n",
    "\n",
    "        ldaModel = build_model(corpus, id2word,data_lemmatized)\n",
    "        pprint('year {} model done!'.format(year))\n",
    "        \n",
    "        save_topic_dataframes(ldaModel,corpus,dataFrameName,year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameName = 'climate_change'\n",
    "years = ['2007','2018','2013']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'subset_data/nlp_2.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb Cell 9\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m run_topics_code_for_subsets(dataFrameName,\u001b[39m'\u001b[39;49m\u001b[39m2013\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb Cell 9\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m year \u001b[39min\u001b[39;00m years:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     fileName \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msubset_data/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(dataFrameName,year)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     corpus, id2word,data_lemmatized \u001b[39m=\u001b[39m get_corpus(fileName)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     pprint(\u001b[39m'\u001b[39m\u001b[39myear \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m corpus done!\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(year))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     ldaModel \u001b[39m=\u001b[39m build_model(corpus, id2word,data_lemmatized)\n",
      "\u001b[1;32m/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb Cell 9\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_corpus\u001b[39m(df):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data_lemmatized \u001b[39m=\u001b[39m preprocess(df)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     id2word \u001b[39m=\u001b[39m corpora\u001b[39m.\u001b[39mDictionary(data_lemmatized)  \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m# Create Corpus \u001b[39;00m\n",
      "\u001b[1;32m/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb Cell 9\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# load daaset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# path = os.getcwd()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# df = pd.read_parquet('{}/project_data/arxiv_climate_change.parquet'.format(path), engine='auto')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m filePath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.parquet\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (dataFrameName)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(filePath)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Remove newline characters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Convert to list \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tfai/Documents/GitHub/wins-topics-network/run_LDA_analysis_for_plots.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m data \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mabstract\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtolist()  \n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m     use_nullable_dtypes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 509\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    510\u001b[0m     path,\n\u001b[1;32m    511\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m    512\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    513\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    514\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    515\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    516\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parquet.py:220\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    218\u001b[0m     to_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39msplit_blocks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    221\u001b[0m     path,\n\u001b[1;32m    222\u001b[0m     kwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mfilesystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    223\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    224\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     pa_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mread_table(\n\u001b[1;32m    228\u001b[0m         path_or_handle, columns\u001b[39m=\u001b[39mcolumns, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    229\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parquet.py:110\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m handles \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    102\u001b[0m     \u001b[39mnot\u001b[39;00m fs\n\u001b[1;32m    103\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[39m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     handles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m    111\u001b[0m         path_or_handle, mode, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m     fs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     path_or_handle \u001b[39m=\u001b[39m handles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    869\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'subset_data/nlp_2.parquet'"
     ]
    }
   ],
   "source": [
    "run_topics_code_for_subsets(dataFrameName,'2013')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
