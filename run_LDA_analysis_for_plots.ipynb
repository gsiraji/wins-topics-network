{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tfai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2023-05-05 11:20:48.938752: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# from https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "from pprint import pprint# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel# spaCy for preprocessing\n",
    "import spacy# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import parquet\n",
    "import os\n",
    "\n",
    "# Prepare stopwords\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    # Build the bigram\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    # Faster way to get a sentence clubbed as a bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    # Build the trigram models\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    # See trigram example\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    # nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: DeprecationWarning: invalid escape sequence '\\s'\n",
      "<>:25: DeprecationWarning: invalid escape sequence '\\s'\n",
      "/var/folders/nk/cmv723pj6n38_mwnlhj_7z3m0000gp/T/ipykernel_98016/81130803.py:25: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words and cleanup the text\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            \n",
    "    #deacc=True removes punctuations\n",
    "\n",
    "def preprocess(dataFrameName):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "    # load daaset\n",
    "    # path = os.getcwd()\n",
    "    # df = pd.read_parquet('{}/project_data/arxiv_climate_change.parquet'.format(path), engine='auto')\n",
    "    \n",
    "    \n",
    "    filePath = '%s.parquet' % (dataFrameName)\n",
    "\n",
    "    df = pd.read_parquet(filePath)\n",
    "\n",
    "    # Remove newline characters\n",
    "    # Convert to list \n",
    "    data = df.abstract.values.tolist()  \n",
    "    # Remove new line characters \n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]  \n",
    "    # Remove distracting single quotes \n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]  \n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    # Call preprocessing functions in order\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "    \n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "    # print(data_lemmatized[:1])\n",
    "\n",
    "    return data_lemmatized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary and Corpus needed for Topic Modeling\n",
    "    # Create Dictionary \n",
    "\n",
    "def get_corpus(df):\n",
    "    data_lemmatized = preprocess(df)\n",
    "    id2word = corpora.Dictionary(data_lemmatized)  \n",
    "        # Create Corpus \n",
    "    texts = data_lemmatized  \n",
    "        # Term Document Frequency \n",
    "    corpus = [id2word.doc2bow(text) for text in texts]  \n",
    "\n",
    "    return(corpus, id2word,data_lemmatized)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(df,corpus, id2word,data_lemmatized):\n",
    "    # Building topic model\n",
    "    \n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=16, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1, # determines how often the model parameters should be updated\n",
    "                                            chunksize=100, # the number of documents to be used in each training chunk\n",
    "                                            passes=10, # the total number of training passes\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "\n",
    "    # Print the keyword of topics\n",
    "    # pprint(lda_model.print_topics())\n",
    "    # doc_lda = lda_model[corpus]\n",
    "    # # Evaluate topic models\n",
    "\n",
    "    # # Compute model Perplexity and Coherence score\n",
    "    # # Compute Perplexity\n",
    "    # # print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "    # # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # # Compute Coherence Score\n",
    "    # coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    # coherence_lda = coherence_model_lda.get_coherence()\n",
    "    # print('\\nCoherence Score: ', coherence_lda)\n",
    "    #Higher the topic coherence, the topic is more human interpretable.\n",
    "    return(lda_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, id2word,data_lemmatized = get_corpus('subset_data/nlp_2007')\n",
    "ldaModel = build_model('arxiv_climate_change',corpus, id2word,data_lemmatized)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.040*\"low\" + 0.030*\"selection\" + 0.027*\"restrict\" + 0.026*\"series\" + 0.021*\"tag\" + 0.020*\"factor\" + 0.019*\"investigate\" + 0.017*\"suitable\" + 0.016*\"insight\" + 0.015*\"bad\"'),\n",
       " (1,\n",
       "  '0.056*\"case\" + 0.040*\"planning\" + 0.031*\"learn\" + 0.021*\"category\" + 0.020*\"use\" + 0.020*\"structure\" + 0.019*\"develop\" + 0.019*\"methodology\" + 0.019*\"demonstrate\" + 0.017*\"problem\"'),\n",
       " (2,\n",
       "  '0.046*\"language\" + 0.031*\"natural\" + 0.025*\"speech\" + 0.024*\"processing\" + 0.018*\"paper\" + 0.016*\"application\" + 0.016*\"use\" + 0.016*\"information\" + 0.015*\"system\" + 0.015*\"semantic\"'),\n",
       " (3,\n",
       "  '0.046*\"line\" + 0.024*\"direct\" + 0.021*\"mathematical\" + 0.019*\"deadline\" + 0.017*\"table\" + 0.017*\"relation\" + 0.014*\"wiktionary\" + 0.012*\"large\" + 0.010*\"number\" + 0.009*\"definition\"'),\n",
       " (4,\n",
       "  '0.048*\"model\" + 0.045*\"base\" + 0.030*\"probabilistic\" + 0.029*\"method\" + 0.027*\"estimation\" + 0.026*\"estimate\" + 0.026*\"parameter\" + 0.025*\"probability\" + 0.019*\"problem\" + 0.018*\"give\"'),\n",
       " (5,\n",
       "  '0.030*\"base\" + 0.025*\"language\" + 0.022*\"module\" + 0.020*\"parse\" + 0.019*\"rule\" + 0.017*\"paper\" + 0.015*\"datum\" + 0.015*\"research\" + 0.014*\"evaluation\" + 0.014*\"programme\"'),\n",
       " (6,\n",
       "  '0.059*\"datum\" + 0.053*\"grammar\" + 0.048*\"incomplete\" + 0.037*\"present\" + 0.031*\"context\" + 0.021*\"programming\" + 0.020*\"specific\" + 0.017*\"automate\" + 0.017*\"language\" + 0.016*\"speak\"'),\n",
       " (7,\n",
       "  '0.044*\"cluster\" + 0.020*\"type\" + 0.013*\"gain\" + 0.013*\"german\" + 0.009*\"become\" + 0.007*\"feature\" + 0.006*\"french\" + 0.006*\"baseline\" + 0.006*\"clustering\" + 0.006*\"core\"'),\n",
       " (8,\n",
       "  '0.028*\"syntax\" + 0.023*\"foreign\" + 0.023*\"simplistic\" + 0.022*\"service\" + 0.020*\"advance\" + 0.018*\"ambiguity\" + 0.013*\"computer\" + 0.011*\"situation\" + 0.010*\"biomedical\" + 0.009*\"script\"'),\n",
       " (9,\n",
       "  '0.023*\"task\" + 0.020*\"sense\" + 0.018*\"datum\" + 0.017*\"model\" + 0.016*\"system\" + 0.016*\"nlp\" + 0.015*\"classification\" + 0.015*\"use\" + 0.015*\"discourse\" + 0.014*\"representation\"'),\n",
       " (10,\n",
       "  '0.103*\"word\" + 0.048*\"model\" + 0.033*\"combination\" + 0.030*\"similarity\" + 0.029*\"language\" + 0.029*\"method\" + 0.025*\"task\" + 0.021*\"disambiguation\" + 0.019*\"likelihood\" + 0.018*\"determine\"'),\n",
       " (11,\n",
       "  '0.063*\"failure\" + 0.052*\"cbp\" + 0.045*\"goal\" + 0.027*\"induce\" + 0.025*\"error\" + 0.022*\"success\" + 0.022*\"evaluation\" + 0.021*\"current\" + 0.020*\"make\" + 0.011*\"help\"'),\n",
       " (12,\n",
       "  '0.026*\"retrieval\" + 0.023*\"document\" + 0.022*\"search\" + 0.018*\"large\" + 0.017*\"web\" + 0.016*\"class\" + 0.015*\"base\" + 0.014*\"semantic\" + 0.013*\"problem\" + 0.013*\"part\"'),\n",
       " (13,\n",
       "  '0.037*\"model\" + 0.030*\"belief\" + 0.028*\"present\" + 0.020*\"approach\" + 0.019*\"test\" + 0.018*\"general\" + 0.015*\"paper\" + 0.014*\"language\" + 0.013*\"view\" + 0.013*\"use\"'),\n",
       " (14,\n",
       "  '0.029*\"phenomenon\" + 0.022*\"roget\" + 0.010*\"wordnet\" + 0.010*\"neutrino\" + 0.010*\"charge\" + 0.010*\"pair\" + 0.009*\"question\" + 0.008*\"flux\" + 0.008*\"particle\" + 0.007*\"linguist\"'),\n",
       " (15,\n",
       "  '0.050*\"learn\" + 0.045*\"memory\" + 0.042*\"base\" + 0.031*\"task\" + 0.027*\"phrase\" + 0.026*\"language\" + 0.024*\"instance\" + 0.023*\"accuracy\" + 0.022*\"experiment\" + 0.022*\"training\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaModel.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
